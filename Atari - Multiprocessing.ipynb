{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gym[atari,accept-rom-license] atari-py\n",
    "!pip install moviepy\n",
    "# in case you get a error saying: \"cannot import name 'NotRequired' from 'typing_extensions'\". Do these:\n",
    "# !pip uninstall tensorflow-gpu\n",
    "#!pip install --upgrade typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128 # Transcations to be sampled from replay buffer\n",
    "GAMMA = 0.99 # discount factor\n",
    "TAU = 0.005#Updation rate\n",
    "LR = 1e-4#learning rate\n",
    "EPS_START = 0.9#beginning value of epsilon\n",
    "EPS_END = 0.05 # epsilon value after decay\n",
    "EPS_DECAY = 1000# Rate of epsilon decay in epsilon greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Pretrained model utils\n",
    "from play import *\n",
    "\n",
    "# Note: Depending on what we need in the assignment, we can use various gym wrappers, Monitor wrapper, video recording wrapper, etc\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "\n",
    "# Using Atari preprocessing requires us to use NoFramesskip environment instead of the usual Breakout-v4\n",
    "# env_ = \n",
    "# env = AtariPreprocessing(env_, scale_obs=False)  # auto skips 4 frames, converts grayscale\n",
    "\n",
    "num_envs = 6# CHANGE THIS FOR NUMBER OF ENVIRONMENTS ------------------\n",
    "\n",
    "# Introducing Frame stack to get vel, acc, etc\n",
    "# env = gym.vector.AsyncVectorEnv([\n",
    "env = gym.vector.AsyncVectorEnv(\n",
    "    [lambda: FrameStack(AtariPreprocessing(\n",
    "                            gym.make(\"BreakoutNoFrameskip-v4\",render_mode='rgb_array'), \n",
    "                            scale_obs=False), \n",
    "                        num_stack=4\n",
    "                        ) for _ in range(num_envs)])\n",
    "# env = gym.wrappers.RecordEpisodeStatistics(env_, deque_size=1000)\n",
    "\n",
    "# record_every = 100\n",
    "\n",
    "#record video for every even episodes\n",
    "# env = gym.wrappers.RecordVideo(env, 'video', episode_trigger = lambda x: x % record_every == 0)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interaction = namedtuple('Interaction',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append(Interaction(state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space[0].n\n",
    "# Get the number of state observations\n",
    "states, infos = env.reset()\n",
    "n_observations = len(states[0])\n",
    "\n",
    "def getEpsilon():\n",
    "    return EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    batch_size = states.shape[0]\n",
    "    steps_done += 1\n",
    "    if random.random() > getEpsilon():\n",
    "        with torch.no_grad():\n",
    "            return policy_net(states).max(1)[1].unsqueeze(1)\n",
    "    else:\n",
    "        return torch.tensor(env.action_space.sample(), device=device, dtype=torch.long).unsqueeze(1).expand(batch_size, 1)\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "            \n",
    "def plot_rewards(show_result=False, clipAt=10000, saveFig=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor([x if x<clipAt else clipAt for x in rewards_episodes], dtype=torch.float)[:450]\n",
    "    if show_result:\n",
    "        plt.title('SpaceInvaders Within-Game RL')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('SpaceInvaders Within-Game RL')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    means = torch.zeros(99)#torch.tensor([durations_t[:i+1].mean() for i in range(min(len(durations_t), 100))])\n",
    "    if len(durations_t)>=100:  \n",
    "        means_later = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((means, means_later))\n",
    "    plt.plot(means.numpy())\n",
    "    if saveFig:\n",
    "        plt.savefig(f\"{EXP_NAME}/rewards.jpg\")\n",
    "        \n",
    "    plt.close()\n",
    "\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            return\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "            \n",
    "def plot_losses(show_result=False, saveFig=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor([x for x in episode_losses], dtype=torch.float)[:450]\n",
    "    if show_result:\n",
    "        plt.title('SpaceInvaders Within-Game RL')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('SpaceInvaders Within-Game RL')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    means = torch.zeros(99)#torch.tensor([durations_t[:i+1].mean() for i in range(min(len(durations_t), 100))])\n",
    "    if len(durations_t)>=100:  \n",
    "        means_later = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((means, means_later))\n",
    "    plt.plot(means.numpy())\n",
    "    if saveFig:\n",
    "        plt.savefig(f\"{EXP_NAME}/losses.jpg\")\n",
    "    plt.close()\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            return\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    Interactions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Interaction(*zip(*Interactions))\n",
    "\n",
    "    # Mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "#     print(torch.cat(batch.state).shape, batch.state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action).unsqueeze(1)\n",
    "    reward_batch = torch.cat(batch.reward).unsqueeze(1)\n",
    "    \n",
    "#     print(state_batch.shape, action_batch.shape, reward_batch.shape, non_final_next_states.shape)\n",
    "\n",
    "    # Compute Q(s_t, a)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # V(s_{t+1})\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_checkpoint(fpath, device=\"cpu\"):\n",
    "    fpath = Path(fpath)\n",
    "    with fpath.open(\"rb\") as file:\n",
    "        with GzipFile(fileobj=file) as inflated:\n",
    "            return torch.load(inflated, map_location=device)\n",
    "\n",
    "path = \"models/DQN_modern/Breakout/2/model_50000000.gz\"\n",
    "\n",
    "pretrained_model = AtariNet(env.action_space[0].n, distributional=\"C51_\" in path)\n",
    "ckpt = _load_checkpoint(path)\n",
    "pretrained_model.load_state_dict(ckpt[\"estimator_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Atari(nn.Module):\n",
    "    \"\"\" Estimator used by DQN-style algorithms for ATARI games.\n",
    "        Works with DQN, M-DQN and C51.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_no, distributional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_no = out_size = action_no\n",
    "        self.distributional = distributional\n",
    "\n",
    "        # configure the support if distributional\n",
    "        if distributional:\n",
    "            support = torch.linspace(-10, 10, 51)\n",
    "            self.__support = nn.Parameter(support, requires_grad=False)\n",
    "            out_size = action_no * len(self.__support)\n",
    "\n",
    "        # get the feature extractor and fully connected layers\n",
    "#         self.__features = nn.Sequential(\n",
    "#             nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "#         def _load_checkpoint(fpath, device=\"cpu\"):\n",
    "#             fpath = Path(fpath)\n",
    "#             with fpath.open(\"rb\") as file:\n",
    "#                 with GzipFile(fileobj=file) as inflated:\n",
    "#                     return torch.load(inflated, map_location=device)\n",
    "        \n",
    "#         path = \"models/DQN_modern/Breakout/2/model_50000000.gz\"\n",
    "        \n",
    "#         pretrained_model = AtariNet(env.action_space.n, distributional=\"C51_\" in path)\n",
    "#         ckpt = _load_checkpoint(path)\n",
    "#         pretrained_model.load_state_dict(ckpt[\"estimator_state\"])\n",
    "    \n",
    "        self.__features = pretrained_model._AtariNet__features\n",
    "        for param in self.__features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.__head = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(inplace=True), nn.Linear(512, out_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clamp(0, 255).to(torch.uint8)\n",
    "        assert x.dtype == torch.uint8, \"The model expects states of type ByteTensor\"\n",
    "        x = x.float().div(255)\n",
    "\n",
    "        x = self.__features(x)\n",
    "        qs = self.__head(x.view(x.size(0), -1))\n",
    "\n",
    "        if self.distributional:\n",
    "            logits = qs.view(qs.shape[0], self.action_no, len(self.__support))\n",
    "            qs_probs = torch.softmax(logits, dim=2)\n",
    "            return torch.mul(qs_probs, self.__support.expand_as(qs_probs)).sum(2)\n",
    "        return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN_Atari(n_actions).to(device)\n",
    "target_net = DQN_Atari(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "rewards_episodes = []\n",
    "episode_durations = []\n",
    "\n",
    "i_episode = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if torch.cuda.is_available():\n",
    "    total_episodes = 10000# Default was 600\n",
    "else:\n",
    "    total_episodes = 50\n",
    "\n",
    "MEMORY_SIZE = 10000\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "while i_episode < total_episodes:\n",
    "    i_episode += 1\n",
    "    # Initialize the environment and get it's state\n",
    "    states, infos = env.reset()\n",
    "    print(f\"In {i_episode}th episode--------------> \")\n",
    "    state = torch.tensor(states, dtype=torch.float32, device=device)#.unsqueeze(0)\n",
    "    episode_reward = torch.zeros(num_envs, device=device)\n",
    "    dones = [False for _ in range(num_envs)]\n",
    "    for t in count():\n",
    "        action = select_action(state)# [1, 1, NUM_ENVS]. Not sure why this size?\n",
    "        actions = action.view(-1, 1)\n",
    "#         print(action.shape, actions.shape, state.shape)\n",
    "        \n",
    "        obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        old_dones = dones\n",
    "        dones = [termi or trunc or dones[i] for i, (termi, trunc) in enumerate(zip(terminated, truncated))]\n",
    "\n",
    "        episode_reward += rewards\n",
    "        # Store Interactions in memory\n",
    "        for i in range(num_envs):\n",
    "            if not old_dones[i]:\n",
    "                memory.push(state[i].unsqueeze(0), actions[i], obs[i].unsqueeze(0) if not dones[i] else None, torch.tensor([rewards[i]], device=device))\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = obs\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        # Record statistics and print progress\n",
    "        if any(dones):\n",
    "            for i, done in enumerate(dones):\n",
    "                if done and not old_dones[i]:#i.e. in this exact frame, the game ended for ith environment. After this, the condition will be false\n",
    "                    episode_durations.append(t + 1)\n",
    "                    rewards_episodes.append(episode_reward[i].item())\n",
    "                    print(f\"Episode {i_episode}/{total_episodes}, Env {i+1}/{num_envs}, Duration {t+1}, \"\n",
    "                              f\"Reward {episode_reward[i].item():.2f}\")\n",
    "        if dones==[True for _ in range(num_envs)]:   \n",
    "            # Save the model weights\n",
    "            if i_episode % 100 == 0:\n",
    "                torch.save(target_net.state_dict(), \"target_net.pth\")\n",
    "                torch.save(policy_net.state_dict(), \"policy_net.pth\")\n",
    "            if i_episode<20 or i_episode%100==0:\n",
    "                plot_rewards(show_result=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_durations(show_result=True)\n",
    "plot_rewards(show_result=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
